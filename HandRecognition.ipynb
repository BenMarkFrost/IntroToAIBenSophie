{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Intelligence \n",
    "## Main Project Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benjamin Frost & Sophie Chalklin \n",
    "#### December 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses a webcam to classify the gesture of a hand.\n",
    "\n",
    "A fist and the index pointing finger can be classified.\n",
    "\n",
    "It is recommended to point your webcam to a surface with a plain background, and hold your hand directly upward.\n",
    "\n",
    "#### Prerequisites\n",
    "\n",
    "##### Libraries\n",
    "\n",
    "This notebook was developed with the following library versions:\n",
    "\n",
    "opencv-contrib-python==4.4.0.46<br />\n",
    "numpy==1.19.3<br />\n",
    "Keras==2.4.3<br />\n",
    "tensorflow-gpu==2.4.0<br />\n",
    "imutils==0.5.3\n",
    "\n",
    "##### Model\n",
    "\n",
    "The model HandModelV# accompanies this notebook and must be placed in the folder \"Model Versions/\" relative to this notebook. By default, the notebook loads in the most recent model in this folder, however during development varying levels of success were found with the models so I would recommmend trying different models to see which work best.\n",
    "\n",
    "Also accompanying this notebooks is the notebook used to traing the CNN classifier for this project.\n",
    "\n",
    "#### Thanks and partial credit for some data filtering code is due to BhaskarP9 from https://www.instructables.com/Opencv-Python-Hand-Detection-and-Tracking/. Code taken from this website is referenced with the @BhaskarP9 tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import keras\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model Model Versions/HandModelV18.h5\n"
     ]
    }
   ],
   "source": [
    "def loadModel():\n",
    "    \n",
    "    version = 0\n",
    "    modelDir = \"Model Versions/HandModelV\"\n",
    "\n",
    "    # This method always gets the most up to date model.\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            version = version + 1\n",
    "            f = open(modelDir + str(version) + \".h5\", 'r')\n",
    "            f.close()\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "    model = keras.models.load_model(modelDir + str(version-1) + \".h5\")\n",
    "    \n",
    "    print(\"Using model \" + modelDir + str(version-1) + \".h5\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.setUseOptimized(True);\n",
    "cv2.setNumThreads(4);\n",
    "\n",
    "version = 1\n",
    "dir = \"HandsData/One/3one\"\n",
    "\n",
    "def saveImage(handBox):\n",
    "\n",
    "    # This simple function saves the images to be processed at a later date by the image classifier.\n",
    "    \n",
    "    global version\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            f = open(dir + str(version) + \".jpg\", 'r')\n",
    "            f.close()\n",
    "            version = version + 1\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    try:\n",
    "        cv2.imwrite(dir + str(version) + \".jpg\", handBox)\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeX = 60\n",
    "sizeY = 100\n",
    "\n",
    "def resize(image):\n",
    "    \n",
    "    # Since size of the box around the hand can vary each frame by a great deal, \n",
    "    # this function ensures that image sizes are standardised to 60 by 100.\n",
    "    # This is important since the classifier needs all input images to be the same size.\n",
    "    \n",
    "    \n",
    "    # Converting from the old BGR to RGB\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    h, w, ch = image.shape\n",
    "    \n",
    "    \n",
    "    #Creating the base of the new standardised image\n",
    "    \n",
    "    zeros = np.zeros((sizeY, sizeX, 3))\n",
    "    \n",
    "    tempImage = image.copy()\n",
    "    \n",
    "    \n",
    "    # Resize the image to be just as tall as the standardised dimensions.\n",
    "    \n",
    "    image = imutils.resize(tempImage, height=sizeY)\n",
    "    \n",
    "    \n",
    "    # If this resizing process results in an image that has a width greater than 60,\n",
    "    # instead resize the image to be just as wide as the standardised dimensions.\n",
    "    \n",
    "    if image.shape[1] > sizeX:\n",
    "        image = imutils.resize(tempImage, width=sizeX)\n",
    "    \n",
    "    # One of these resizing options will result in an image that either has a black \n",
    "    # portion at the top or at the right hand side.\n",
    "    \n",
    "    zeros[:image.shape[0], :image.shape[1]] = image\n",
    "    \n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputText = \"\"\n",
    "combinedOutput = []\n",
    "\n",
    "def classifyHand(handBox):\n",
    "    \n",
    "    # Acceses the globally declared variables\n",
    "    \n",
    "    global outputText, combinedOutput\n",
    "    \n",
    "    # Sometimes this resizing method fails, and I cannot explain it.\n",
    "    # It only fails once out of every 100 or 200 frames, so it's easy\n",
    "    # to hide the error and return last frame's results.\n",
    "    \n",
    "    try :\n",
    "        handBox = resize(handBox)\n",
    "    except:\n",
    "        print(\"Resizing error\") \n",
    "        return outputText, combinedOutput\n",
    "    \n",
    "    handBox = np.array(handBox)    \n",
    "        \n",
    "    # Using the resized image to predict the gesture of the hand.\n",
    "    \n",
    "    pred_hot = model.predict(np.expand_dims(handBox, axis=0))[0]\n",
    "\n",
    "    \n",
    "    if pred_hot[0] > pred_hot[1]:\n",
    "        outputText = \"Index \"\n",
    "    else:\n",
    "        outputText = \"Fist \"\n",
    "    \n",
    "    \n",
    "    combinedOutput = [\"Fist: \" + str(pred_hot[1]), \"Index \" + str(pred_hot[0])]\n",
    "    \n",
    "    return outputText, combinedOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeBackground(frame):\n",
    "    \n",
    "    # @BhaskarP9\n",
    "    \n",
    "    # Separate the foreground and the background in the webcam image.\n",
    "    \n",
    "    background = cv2.createBackgroundSubtractorMOG2(history = 500, varThreshold = 16, detectShadows= False)\n",
    "    bgMask = background.apply(frame)\n",
    "    \n",
    "    # Size of erosion operation to perform.\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    \n",
    "    # Erosion makes a pixel 1 if all the pixels in the kernel around \n",
    "    # the pixel in question are also 1. Otherwise, it is 0.\n",
    "    # This contracts the boundaries of the mask making it more accurate and tighter.\n",
    "    bgMask = cv2.erode(bgMask, kernel, iterations=1)\n",
    "\n",
    "    return cv2.bitwise_and(frame, frame, mask = bgMask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def captureAndCleanData():\n",
    "    \n",
    "    # @BhaskarP9\n",
    "    \n",
    "    # Getting the current frame from the webcam\n",
    "    \n",
    "    ret, frame = camera.read()\n",
    "\n",
    "    # In early tests, without this filtering the video is far less smooth and tracking is worse.\n",
    "    \n",
    "    frame = cv2.bilateralFilter(frame, 10, 50, 90)  # Smoothing\n",
    "    mask = removeBackground(frame)\n",
    "\n",
    "    # Converting the masked image to HSV to be able to separate skin tones from the background.\n",
    "    \n",
    "    hsv = cv2.cvtColor(mask, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower = np.array([0,40,80], dtype=\"uint8\")\n",
    "    upper = np.array([20,255,255], dtype=\"uint8\")\n",
    "    \n",
    "    #This mask only keeps the data within the bounds defined above.\n",
    "    \n",
    "    skinMask = cv2.inRange(hsv, lower, upper)\n",
    "    \n",
    "    # Find contours in the image. This creates a continuous line around the hand.\n",
    "    \n",
    "    contours, h = cv2.findContours(skinMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    return frame, contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 60, 100, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 60, 100, 3), dtype=tf.float32, name='conv2d_5_input'), name='conv2d_5_input', description=\"created by layer 'conv2d_5_input'\"), but it was called on an input with incompatible shape (None, 100, 60, 3).\n",
      "Resizing error\n",
      "Resizing error\n",
      "Resizing error\n",
      "Resizing error\n",
      "Resizing error\n",
      "Resizing error\n",
      "Resizing error\n",
      "Resizing error\n",
      "Resizing error\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Resizing error\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n",
      "Too close\n",
      "Resizing error\n"
     ]
    }
   ],
   "source": [
    "camera = cv2.VideoCapture(1)\n",
    "\n",
    "while (True):\n",
    "\n",
    "    # Exit key (If you don't press this to exit then the program closes by crashing)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    frame, contours = captureAndCleanData()\n",
    "    \n",
    "    if len(contours) > 0:\n",
    "        \n",
    "        # Finding the largest area contour, which is most likely the hand.\n",
    "        \n",
    "        areas = []\n",
    "        for contour in contours:\n",
    "            areas.append(cv2.contourArea(contour))\n",
    "        maxCon = max(areas)\n",
    "        res = contours[areas.index(maxCon)]\n",
    "    \n",
    "    \n",
    "        # Finding the rectangle that bounds the contour.\n",
    "        \n",
    "        x, y, w, h = cv2.boundingRect(res)\n",
    "               \n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # Adding padding around the box of interest. This will fail if it is too close to the edge.\n",
    "            \n",
    "            handBox = frame[int(y-h/4):int(y+h*1.25), int(x-w/4):int(x+w*1.25)]\n",
    "            \n",
    "            \n",
    "        except:\n",
    "            \n",
    "            # If the hand is at the edge of the screen no padding is applied\n",
    "            \n",
    "            handBox = frame[y:y+h, x:x+w]\n",
    "            \n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # If the user's hand is too close to the screen, \n",
    "            # the box around the hand that was generated by \n",
    "            # OpenCV will be taller than the frame, which causes this to fail.\n",
    "            \n",
    "            cv2.imshow(\"Box\", handBox)\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            print(\"Too close\")\n",
    "                                \n",
    "            \n",
    "        # saveImage() is used for collecting images for training the classifier. \n",
    "        # We only ever want to be either saving the images or classifying them directly.\n",
    "        \n",
    "        save = False\n",
    "        \n",
    "        if save:\n",
    "            \n",
    "            saveImage(handBox)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Two outputs from the classifier function - a verbose version and a long version.\n",
    "            \n",
    "            outputText, combinedOutput = classifyHand(handBox)\n",
    "\n",
    "            \n",
    "            # Printing the results of the classifier in the top left corner.\n",
    "            # This is looped so that more classes can be added to the model in the future.\n",
    "            \n",
    "            spacing = 25\n",
    "            for i in range(len(combinedOutput)):\n",
    "                cv2.putText(frame, combinedOutput[i], (10, 25 + (i * spacing)), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                \n",
    "            # Adding the label on the hand itself.\n",
    "            \n",
    "            cv2.putText(frame, outputText, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Adding the box around the hand\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "        \n",
    "    cv2.imshow(\"Output\", frame)\n",
    "        \n",
    "    \n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
