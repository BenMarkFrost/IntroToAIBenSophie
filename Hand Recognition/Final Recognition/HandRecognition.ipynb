{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Intelligence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benjamin Frost & Sophie Chalklin \n",
    "#### December 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses a webcam to classify the gesture of a hand.\n",
    "\n",
    "A fist and the index pointing finger can be classified.\n",
    "\n",
    "It is recommended to point your webcam to a surface with a plain background, and hold your hand directly upward.\n",
    "\n",
    "#### Prerequisites\n",
    "\n",
    "##### Libraries\n",
    "\n",
    "This notebook was developed with the following library versions:\n",
    "\n",
    "opencv-contrib-python==4.4.0.46<br />\n",
    "numpy==1.19.3<br />\n",
    "Keras==2.4.3<br />\n",
    "tensorflow-gpu==2.4.0<br />\n",
    "imutils==0.5.3\n",
    "\n",
    "##### Model\n",
    "\n",
    "The model HandModelV# accompanies this notebook and must be placed in the folder \"Model Versions/\" relative to this notebook. By default, the notebook loads in the most recent model in this folder, however during development varying levels of success were found with the models so I would recommmend trying different models to see which work best.\n",
    "\n",
    "Also accompanying this notebooks is the notebook used to traing the CNN classifier for this project.\n",
    "\n",
    "#### Thanks and partial credit for some data filtering code is due to BhaskarP9 from https://www.instructables.com/Opencv-Python-Hand-Detection-and-Tracking/. Code taken from this website is referenced with the @BhaskarP9 tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import keras\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model Model Versions/HandModelV5.h5\n"
     ]
    }
   ],
   "source": [
    "def loadModel():\n",
    "    \n",
    "    version = 0\n",
    "    modelDir = \"Model Versions/HandModelV\"\n",
    "\n",
    "    # This method always gets the most up to date model.\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            version = version + 1\n",
    "            f = open(modelDir + str(version) + \".h5\", 'r')\n",
    "            f.close()\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "    model = keras.models.load_model(modelDir + str(version-1) + \".h5\")\n",
    "    \n",
    "    print(\"Using model \" + modelDir + str(version-1) + \".h5\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.setUseOptimized(True);\n",
    "cv2.setNumThreads(4);\n",
    "\n",
    "version = 1\n",
    "dir = \"../../../../HandsData/OpenCVHandsData/None/none\"\n",
    "\n",
    "def saveImage(handBox):\n",
    "\n",
    "    # This simple function saves the images to be processed at a later date by the image classifier.\n",
    "    \n",
    "    global version\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            f = open(dir + str(version) + \".jpg\", 'r')\n",
    "            f.close()\n",
    "            version = version + 1\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    try:\n",
    "        cv2.imwrite(dir + str(version) + \".jpg\", handBox)\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeX = 60\n",
    "sizeY = 100\n",
    "\n",
    "def resize(image):\n",
    "    \n",
    "    # Since size of the box around the hand can vary each frame by a great deal, \n",
    "    # this function ensures that image sizes are standardised to 60 by 100.\n",
    "    # This is important since the classifier needs all input images to be the same size.\n",
    "    \n",
    "    \n",
    "    # Converting from the old BGR to RGB\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    h, w, ch = image.shape\n",
    "    \n",
    "    \n",
    "    #Creating the base of the new standardised image\n",
    "    \n",
    "    zeros = np.zeros((sizeY, sizeX, 3))\n",
    "    \n",
    "    tempImage = image.copy()\n",
    "    \n",
    "    \n",
    "    # Resize the image to be just as tall as the standardised dimensions.\n",
    "    \n",
    "    image = imutils.resize(tempImage, height=sizeY)\n",
    "    \n",
    "    \n",
    "    # If this resizing process results in an image that has a width greater than 60,\n",
    "    # instead resize the image to be just as wide as the standardised dimensions.\n",
    "    \n",
    "    if image.shape[1] > sizeX:\n",
    "        image = imutils.resize(tempImage, width=sizeX)\n",
    "    \n",
    "    # One of these resizing options will result in an image that either has a black \n",
    "    # portion at the top or at the right hand side.\n",
    "    \n",
    "    zeros[:image.shape[0], :image.shape[1]] = image\n",
    "    \n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputText = \"\"\n",
    "combinedOutput = []\n",
    "\n",
    "def classifyHand(handBox):\n",
    "    \n",
    "    # Acceses the globally declared variables\n",
    "    \n",
    "    global outputText, combinedOutput\n",
    "    \n",
    "    # Sometimes this resizing method fails, and I cannot explain it.\n",
    "    # It only fails once out of every 100 or 200 frames, so it's easy\n",
    "    # to hide the error and return last frame's results.\n",
    "    \n",
    "    try :\n",
    "        handBox = resize(handBox)\n",
    "    except:\n",
    "        print(\"Resizing error\") \n",
    "        return outputText, combinedOutput\n",
    "    \n",
    "    handBox = np.array(handBox)    \n",
    "        \n",
    "    # Using the resized image to predict the gesture of the hand.\n",
    "    \n",
    "    pred_hot = model.predict(np.expand_dims(handBox, axis=0))[0]\n",
    "\n",
    "    \n",
    "    if pred_hot[0] > pred_hot[1]:\n",
    "        outputText = \"Index \"\n",
    "    else:\n",
    "        outputText = \"Fist \"\n",
    "    \n",
    "    \n",
    "    combinedOutput = [\"Fist: \" + str(pred_hot[1]), \"Index \" + str(pred_hot[0])]\n",
    "    \n",
    "    return outputText, combinedOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeBackground(frame):\n",
    "    \n",
    "    # @BhaskarP9\n",
    "    \n",
    "    # Separate the foreground and the background in the webcam image.\n",
    "    \n",
    "    background = cv2.createBackgroundSubtractorMOG2(0,50)\n",
    "\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "\n",
    "    bgMask = background.apply(frame)\n",
    "    bgMask = cv2.erode(bgMask, kernel, iterations=1)\n",
    "\n",
    "    return cv2.bitwise_and(frame, frame, mask = bgMask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def captureAndCleanData():\n",
    "    \n",
    "    # @BhaskarP9\n",
    "    \n",
    "    # Getting the current frame from the webcam\n",
    "    \n",
    "    ret, frame = camera.read()\n",
    "\n",
    "    # In early tests, without this filtering the video is far less smooth and tracking is worse.\n",
    "    \n",
    "    #frame = cv2.bilateralFilter(frame, 10, 65, 120)  # Smoothing\n",
    "    mask = removeBackground(frame)\n",
    "\n",
    "    # Converting the masked image to HSV to be able to separate skin tones from the background.\n",
    "    \n",
    "    hsv = cv2.cvtColor(mask, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower = np.array([0,48,80], dtype=\"uint8\")\n",
    "    upper = np.array([20,255,255], dtype=\"uint8\")\n",
    "    \n",
    "    #This mask only keeps the data within the bounds defined above.\n",
    "    \n",
    "    skinMask = cv2.inRange(hsv, lower, upper)\n",
    "    \n",
    "    # Find contours in the image. This creates a continuous line around the hand.\n",
    "    \n",
    "    contours, h = cv2.findContours(skinMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    return frame, contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 60, 100, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 60, 100, 3), dtype=tf.float32, name='conv2d_40_input'), name='conv2d_40_input', description=\"created by layer 'conv2d_40_input'\"), but it was called on an input with incompatible shape (None, 100, 60, 3).\n"
     ]
    }
   ],
   "source": [
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "while (True):\n",
    "\n",
    "    # Exit key (If you don't press this to exit then the program closes by crashing)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    frame, contours = captureAndCleanData()\n",
    "    \n",
    "    if len(contours) > 0:\n",
    "        \n",
    "        # Finding the largest area contour, which is most likely the hand.\n",
    "        \n",
    "        areas = []\n",
    "        for contour in contours:\n",
    "            areas.append(cv2.contourArea(contour))\n",
    "        maxCon = max(areas)\n",
    "        res = contours[areas.index(maxCon)]\n",
    "    \n",
    "    \n",
    "        # Finding the rectangle that bounds the contour.\n",
    "        \n",
    "        x, y, w, h = cv2.boundingRect(res)\n",
    "               \n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # Adding padding around the box of interest. This will fail if it is too close to the edge.\n",
    "            \n",
    "            handBox = frame[int(y-h/4):int(y+h*1.25), int(x-w/4):int(x+w*1.25)]\n",
    "            \n",
    "            \n",
    "        except:\n",
    "            \n",
    "            # If the hand is at the edge of the screen no padding is applied\n",
    "            \n",
    "            handBox = frame[y:y+h, x:x+w]\n",
    "            \n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # If the user's hand is too close to the screen, \n",
    "            # the box around the hand that was generated by \n",
    "            # OpenCV will be taller than the frame, which causes this to fail.\n",
    "            \n",
    "            cv2.imshow(\"Box\", handBox)\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            print(\"Too close\")\n",
    "        \n",
    "        \n",
    "        # Adding the box around the hand\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "                \n",
    "            \n",
    "        # saveImage() is used for collecting images for training the classifier. \n",
    "        # We only ever want to be either saving the images or classifying them directly.\n",
    "        \n",
    "        save = False\n",
    "        \n",
    "        if save:\n",
    "            \n",
    "            saveImage(handBox)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Two outputs from the classifier function - a verbose version and a long version.\n",
    "            \n",
    "            outputText, combinedOutput = classifyHand(handBox)\n",
    "\n",
    "            \n",
    "            # Printing the results of the classifier in the top left corner.\n",
    "            # This is looped so that more classes can be added to the model in the future.\n",
    "            \n",
    "            spacing = 25\n",
    "            for i in range(len(combinedOutput)):\n",
    "                cv2.putText(frame, combinedOutput[i], (10, 25 + (i * spacing)), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                \n",
    "            # Adding the label on the hand itself.\n",
    "            \n",
    "            cv2.putText(frame, outputText, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "    cv2.imshow(\"Output\", frame)\n",
    "        \n",
    "    \n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
